<?xml version='1.0' encoding='utf-8' ?>
<!DOCTYPE book [
<!ENTITY % BOOK_ENTITIES SYSTEM "First_Steps_Guide.ent">
%BOOK_ENTITIES;
<!ENTITY % sgml.features "IGNORE">
<!ENTITY % xml.features "INCLUDE">
<!ENTITY % DOCBOOK_ENTS PUBLIC "-//OASIS//ENTITIES DocBook Character Entities V4.5//EN" "/usr/share/xml/docbook/schema/dtd/4.5/dbcentx.mod">
%DOCBOOK_ENTS;
]>
<book xmlns="http://docbook.org/ns/docbook"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude">

  <xi:include href="Book_Info.xml"/>
  <xi:include href="Preface.xml"/>

  <!-- Start of real content -->

  <chapter xml:id="First-boot">
    <title>First Boot</title>
    <section xml:id="sec-Qlustar-Initial-Config">
      <title>Running qlustar-initial-config</title>
      <para>
	After the server has booted the newly installed Qlustar OS, log in as root and start
	the post-install configuration process by running the command
	<screen>
/usr/sbin/qlustar-initial-config
	</screen>
      </para>
      <para>
	This will first thoroughly check your network connectivity and then complete the
	installation by executing the remaining configuration steps as detailed below. During
	the package update process you might be asked whether to keep locally modified
	configuration files. In this case always choose the option
      <screen>
keep the local version currently installed.
      </screen>
      </para>
      <task xml:id="task-Remaining-Config-Steps">
	<title>Remaining configuration steps run-through</title>
	<procedure>
	  <para>
	    If your chosen hostname can't be resolved via <firstterm>DNS</firstterm>, you will
	    see a non-fatal error message reminding you that the hostname should be registered
	    in some (external) name service (typically DNS).
	  </para>
	  <step>
	    <title>Cluster name</title>
	    <para>
	      <mediaobject>
		<imageobject><imagedata fileref="images/initial-config-1.png"
		width="85%" format="PNG"/></imageobject>
		<textobject><phrase>
		  Name the cluster
		</phrase></textobject>
		<caption><para>
		  Name the cluster
		</para></caption>
	      </mediaobject>
	      First, you will be asked for the name of the new Qlustar cluster. This can be any
	      string and is used in some places like the <firstterm>slurm</firstterm> or
	      <firstterm>ganglia</firstterm> configuration.
	    </para>
	  </step>
	  <step>
	    <title>NIS Setup</title>
	    <para>
	      <mediaobject>
		<imageobject><imagedata fileref="images/initial-config-3.png"
		width="85%" format="PNG"/></imageobject>
		<textobject><phrase>
		  Setting up NIS
		</phrase></textobject>
		<caption><para>
		  Setting up NIS
		</para></caption>
	      </mediaobject>
	      Next is the setup of the <firstterm>NIS</firstterm> database. Just confirm the
	      suggested NIS server by pressing
	      <keycombo><keycap>Ctrl-D</keycap><keycap>Enter</keycap></keycombo> to proceed.
	    </para>
	  </step>
	  <step>
	    <title>Configuring ssh</title>
	    <para>
	      <mediaobject>
		<imageobject><imagedata fileref="images/initial-config-4.png"
		width="85%" format="PNG"/></imageobject>
		<textobject><phrase>
		  Creating an SSH key
		</phrase></textobject>
		<caption><para>
		  Creating an SSH key
		</para></caption>
	      </mediaobject>
	      An <firstterm>ssh key</firstterm> for the root user is generated next. You can
	      enter an optional <firstterm>pass-phrase</firstterm> for it. This key, will be
	      used to enable login by root to any node of the cluster without specifying a
	      password.
	    </para>
	    <note>
	      <para>
		Be aware, that having a non-empty pass-phrase means, that you will have to
		specify it any time you try to ssh to another host in the cluster. If you
		don't want that, work without a pass-phrase.
	      </para>
	    </note>
	  </step>
	  <step>
	    <title>Configuring Nagios</title>
	    <para>
	      <mediaobject>
		<imageobject><imagedata fileref="images/initial-config-5.png"
		width="85%" format="PNG"/></imageobject>
		<textobject><phrase>
		  Setting the Nagios password
		</phrase></textobject>
		<caption><para>
		  Setting the Nagios password
		</para></caption>
	      </mediaobject>
	      The configuration of <firstterm>Nagios</firstterm> requires you to choose a
	      password for the Nagios admin account. Please type in the password twice.
	    </para>
	  </step>
	  <step>
	    <title>Configuring QluMan</title>
	    <para>
	      <mediaobject>
		<imageobject><imagedata fileref="images/initial-config-7.png"
		width="85%" format="PNG"/></imageobject>
		<textobject><phrase>
		  Bootstrapping QluMan
		</phrase></textobject>
		<caption><para>
		  Bootstrapping QluMan
		</para></caption>
	      </mediaobject>
	      <application>QluMan</application>, the Qlustar management framework (see the
	      QluMan Guide), requires a <firstterm>mysql</firstterm> (mariaDB) database. You
	      will be asked for the password of the QluMan DB user next. After entering it, the
	      QluMan database and configuration settings will be initialized. This can take a
	      while, since a number of OS images and chroots (see <xref
	      linkend="sec-Adding-Software"/>) will be generated during this step.
	    </para>
	  </step>
	  <step>
	    <title>Configuring Slurm</title>
	    <para>
	      <mediaobject>
		<imageobject><imagedata fileref="images/initial-config-9.png"
		width="85%" format="PNG"/></imageobject>
		<textobject><phrase>
		  Setting the Slurm DB password
		</phrase></textobject>
		<caption><para>
		  Setting the Slurm DB password
		</para></caption>
	      </mediaobject>
	      If slurm was selected as the cluster resource manager, its configuration requires
	      the generation of a <firstterm>munge key</firstterm> and the specification of a
	      password for the slurm mysql account. Enter the chosen password twice when asked
	      for it.
	    </para>
	    <note>
	      <para>
		The slurm database daemon is also being configured by this process. Hence, you
		will be ready to take full advantage of the accounting features of slurm.
	      </para>
	    </note>
	  </step>
	  <step>
	    <title>Configuring the virtual Demo Cluster</title>
	    <para>
	      <mediaobject>
		<imageobject><imagedata fileref="images/initial-config-10.png"
		width="85%" format="PNG"/></imageobject>
		<textobject><phrase>
		  Setting up the virtual demo cluster
		</phrase></textobject>
		<caption><para>
		  Setting up the virtual demo cluster
		</para></caption>
	      </mediaobject>
	      If you have chosen to setup some virtual demo nodes during installation, you will
	      be asked for the user name of a test account that can be used to explore the
	      cluster. The account will be generated with the default password for the cluster
	      (see the information on the screen).
	    </para>
	  </step>
	  <step>
	    <title>Setting the MariaDB root password</title>
	    <para>
	      To conclude the configuration procedure, you will be asked to set the password
	      for the MariaDB/MySQL root account. Setting a password here is important. It
	      prevents unauthorized access to the Qlustar or other databases on your head-node.
	    </para>
	  </step>
	</procedure>
      </task>
    </section>
    <section xml:id="sec-Final-Reboot">
      <title>Final Reboot</title>
      <para>
	Reboot again once all the previous steps are complete by pressing
	<keycap>Enter</keycap>. After the head-node is up and running again, test its network
	connectivity by pinging its public IP address (hostname). Do the same for the virtual
	front-end node, if you have chosen to configure one. It should have booted as well
	after the head-node is up and running. You can try to login to it using ssh. A test
	mail should have been sent to the e-mail address(es) you specified during the
	installation. If you didn't receive one, review your settings in
	<filename>/etc/aliases</filename> and/or <filename>/etc/postfix/main.cf</filename>. In
	case some of them are wrong, you can execute
	<screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>dpkg-reconfigure postfix</command>
	</screen>
	to modify them.
      </para>
    </section>
    <section xml:id="sec-Starting-Virtual-Demo-Cluster">
      <title>Starting the virtual Demo Cluster</title>
      <para>
	If you have chosen to configure a virtual demo-cluster, you can start it by executing
	the command:
	<screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>demo-system-start</command>
	</screen>
	and to stop it
	<screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>demo-system-stop</command>
	</screen>
	These commands use the configuration file
	<filename>/etc/qlustar/vm-configs/demo-system.conf</filename>. If you find that the
	(automatically calculated) amount of RAM per VM is not right, you can change the
	variable <varname>CN_MEM</varname> to some other value in that file. The consoles of
	the virtual nodes (and also of the virtual front-end node if you chose to set one up)
	are accessible in a screen session. Type
	<screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>console-fe-vm</command>
	</screen>
	to attach to the console session of the virtual FE node and
	<screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>console-demo-vms</command>
	</screen>
	to attach to the console sessions of the virtual demo cluster nodes. Note that the
	screen command character is <keycap>Ctrl-t</keycap>. To detach from the screen session,
	type <keycombo><keycap>Ctrl-t</keycap><keycap>d</keycap></keycombo>, to switch to the
	next/previous screen type
	<keycombo><keycap>Ctrl-t</keycap><keycap>n</keycap></keycombo> /
	<keycombo><keycap>Ctrl-t</keycap><keycap>p</keycap></keycombo>. More details on the
	usage of screen (or byobu, the Debian customized version we use) are available in the
	corresponding <literal>man pages</literal>. To check whether all nodes are up and
	running, type
	<screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>dsh -a uptime</command>
	</screen>
	<command>dsh</command> or <command>pdsh</command> can be used to execute arbitrary
	commands on groups of nodes. Check their <literal>man pages</literal> and the
	corresponding section in the QluMan guide for further information.
      </para>
    </section>
    <section xml:id="sec-Installed-Services">
      <title>Installed Services</title>
      <para>
	At this stage, the following services are configured and running on your head-node:
	<itemizedlist>
	  <listitem>
	    <para>
	      <application>Nagios3</application> (monitoring/alerts) with its web interface at
	      http://<replaceable>headnode</replaceable>/nagios3/. Login as user
	      <literal>nagiosadmin</literal> with the password you specified previously.
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      <application>Ganglia</application> (monitoring) at
	      http://<replaceable>headnode</replaceable>/ganglia/
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      <application>DHCP/ATFTP</application> boot services
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      <application>NTP</application> time server as client and server
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      <application>NFS</application>-Server with exports defined in
	      <filename>/etc/exports</filename>
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      Depending on your choice of software packages: <application>Slurm</application>
	      (DB + control daemon), <application>Torque</application> server,
	      <application>Corosync</application> (HA), <application>Munge</application>
	      (authentification for slurm/torque). Note that among the latter only slurm and
	      munge are configured automatically during installation. Torque and Corosync
	      require a manual configuration.
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      <application>NIS</application> server
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      Mail service <application>Postfix</application>
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      <application>MariaDB</application> server (mysql fork)
	    </para>
	  </listitem>
	  <listitem>
	    <para>
	      <application>QluMan</application> server (Qlustar Management)
	    </para>
	  </listitem>
	</itemizedlist>
	<note>
	  <para>
	    Please note, that you shouldn't install the Ubuntu MySQL server packages on
	    the head-node, since QluMan requires <link
	    xlink:href="https://mariadb.org/___blank___">MariaDB</link> and packages of the
	    latter conflict with the MySQL packages. MariaDB is a complete and compatible
	    substitute for MySQL.
	  </para>
	</note>
      </para>
    </section>
    <section xml:id="sec-Adding-Software">
      <title>Adding Software</title>
      <para>
	As explained <link
	xlink:href="../QluMan_Guide/index.html#sec-UnionFS-Chroots___blank___">
	elsewhere</link>, the RAM-based root file-system of a Qlustar compute/storage node can
	be supplemented by a global NFS-exported chroot to allow access to software not already
	contained in the boot images themselves. During installation, one chroot per selected
	<link
	xlink:href="../Installation_Guide/index.html#Edge-Platform-Selection___blank___">edge
	platform</link> was automatically created. The chroots are located at
	<filename>/srv/apps/chroots/<replaceable>chroot name</replaceable></filename>, where
	<replaceable>chroot name</replaceable> would be xenial. Each of them contains
	a full-featured installation of the corresponding Qlustar edge platform. To change into
	a chroot, convenience bash shell aliases of the form chroot-<replaceable>chroot
	name</replaceable> are defined for the root user on the head-node. You may use them as
	follows (e.g. for Ubuntu/Xenial, if it was selected at install):
	<screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>chroot-xenial</command>
	</screen>
	Once you're inside a chroot, you can use the standard Debian/Ubuntu tools to control
	its software packages, e.g.
	<screen>
<prompt>(xenial) 0 root@cl-head ~ #</prompt>
<command>apt update</command>
<prompt>(xenial) 0 root@cl-head ~ #</prompt>
<command>apt dist-upgrade</command>
<prompt>(xenial) 0 root@cl-head ~ #</prompt>
<command>apt install <replaceable>package</replaceable></command>
<prompt>(xenial) 0 root@cl-head ~ #</prompt>
<command>exit</command>
	</screen>
	The nice thing about this mechanism is that software from packages installed in a
	particular chroot will be available instantaneously on all compute/storage nodes that
	are configured to use that chroot.
      </para>
      <important>
	<para>
	  There is usually no need to install additional packages on the head-node itself (only
	  in the chroot). Software packages installed directly on the head-node will not be
	  visible cluster-wide.
      </para>
      </important>
    </section>
    <section xml:id="sec-Running-QluMan">
      <title>Running the Cluster Manager QluMan</title>
      <section xml:id="sec-QluMan-Token">
	<title>Generating a one time token for the first admin login</title>
      <para>
	The Qlustar management GUI <application>qluman-qt</application> uses public/private
	keys for both encryption and authentication of its connection with the QluMan server
	processes. For this to work, there needs to be an exchange of public keys between the
	GUI client and the QluMan server. Later this can be done by a user with admin role via
	the GUI. But for the first admin login, it must be accomplished using a root shell on
	the head-node as follows:
	<screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>qluman-cli --gencert</command>
Generating one-time login token for user 'admin':
Cluster  = QL
Hostname = beosrv-c
Port     = 6001
Pubkey   = b'T)5o]@hsjB2qyY>eb:7)8@BA?idMf>kh%^cRhV/#'
Enter new pin for one-time token: 
Server infos and one-time login token for user 'admin':
---[ CUT FROM HERE ]---
00000191c2MAAcMuyCNQR0DPILx-y-BLCHpITepvG7R3I6452Cdqiu98u4PsM1VWFGqEAG
V8YN9K5kyJKHtQHGTB1JqZIwt4q0PLArnyNmhCkGLS6VxWWBDtBB9_dGPqLH4OeQ7sZ725
6XDGgrKo4Dldc_wuCALegczjYV8oc_yZ07X0oIYlzhDlDpk-hTm5bfW8_x904YF0wcv-G-
nK1ztRg854O7pC_p1YpEJuzWFqWv0e7ffi-ZgkxwfdGGKF3imp4d9yGY4h6Ixdn8TLG2gk
Z4XQ4dymvSO9hp8mUabfq7prVUOTYeChB2pOrom8XSQxjOoe4Yll5yv6da_CdGq50KrO8Q
C12Z4Pz2eSbvqXbo7c7DdLRjMc0v0Km3WyljgdsDYbKC5iT75Bgryc
---[ TO HERE ]---
	</screen>
	The token can also be saved directly to a file using the <parameter>-o
	&lt;filename&gt;</parameter> option. The user the token is for can be specified by
	the <parameter>-u &lt;username&gt;</parameter> option like this:
	<screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>qluman-cli --gencert -u admin -o token</command>
Generating one-time login token for user 'admin':
Cluster  = QL
Hostname = beosrv-c
Port     = 6001
Pubkey   = b'T)5o]@hsjB2qyY>eb:7)8@BA?idMf>kh%^cRhV/#'
Enter new pin for one-time token: 
Server infos and one-time login token for user 'admin' saved as 'token'
	</screen>
	The server infos and one-time login token are protected by the pin you just
	entered. This is important when the data is sent via unencrypted channels (e.g. email
	or chat programs) to users or when it is stored on a shared filesystem like NFS. The
	pin does not need to be a strong password. It is only used to make it non-trivial to
	use an intercepted token.
      </para>
      <note>
	<para>
	  The token can only be used once. So once you use it yourself, it becomes useless to
	  anybody else. On the other hand, if somebody intercepts the token, guesses the pin
	  and uses it for a connection, it will no longer work for you. If that happened, you'd
	  know something went wrong.
	</para>
      </note>
      </section>
      <section xml:id="sec-Enclosure-View">
	<title>Starting the QluMan GUI</title>
	<para>
	  Per default, the Qlustar management GUI <application>qluman-qt</application> is not
	  installed on any node of the cluster. This is because the installation on the
	  head-node (or a chroot) pulls and installs a lot of other packages that qluman-qt
	  depends on, which will slow down updates. If you have the possibility, <link
	  linkend="sec-QluMan-Workstation">install qluman-qt on your workstation</link> and
	  work from there. If you still like to have it available on the head-node, just
	  install it there like any other package:
	  <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>apt install qluman-qt</command>
	  </screen>
	  Then you can launch qluman-qt remotely on the head-node, per ssh (with X11 forwarding
	  enabled / <parameter>-X</parameter> option) as follows:
	  <screen>
<prompt>0 user@workstation ~ $</prompt>
<command>ssh -X root@servername qluman-qt</command>
	  </screen>
	  This should bring up the <firstterm>Management Console</firstterm>.  Using the
	  one-time token generated as explained above, you will now be able to add the cluster
	  to the list of available connections. (Details about this are explained in the <link
	  xlink:href="../QluMan_Guide/index.html#sec-add-cluster___blank___">QluMan
	  Guide</link>).
	</para>
      </section>
      <section xml:id="sec-QluMan-Workstation">
	<title>Running the QluMan GUI on a workstation</title>
	<para>
	  If your workstation runs one of the edge platforms currently supported by Qlustar, or
	  if it allows executing containers, you can install/run the QluMan GUI directly
	  there. This is recommended, since the <emphasis role="bold">responsiveness</emphasis>
	  of a GUI, that is locally started, is a lot better as compared to one that is running
	  via remote X11. We explain the two alternatives below.
	</para>
	<section xml:id="sec-QluMan-Workstation-package">
	  <title>Installing/running using the Debian package</title>
	  <para>
	    To install the qluman-qt package on your Ubuntu workstation, you need to add the
	    correct Qlustar repository to your <literal>apt sources list</literal>. This can be
	    accomplished by executing the following as root on your workstation.
	    <screen>
<prompt>0 root@workstation ~ #</prompt>
<command>dpkg -l software-properties-common > /dev/null 2>&amp;1 || apt install software-properties-common</command>
<prompt>0 root@workstation ~ #</prompt>
<command>gpg --no-default-keyring --primary-keyring /etc/apt/trusted.gpg --recv-keys E6BA110F3C0BC307</command>
	    </screen>
	    The second one should have imported the <firstterm>Qlustar PGP archive
	    key</firstterm>, and must output a line like:
	    <screen>
gpg: key 3C0BC307: public key "Q-Leap Networks (automatic archive key) &lt;info@q-leap.com>" imported
	    </screen>
	  </para>
	  <note>
	    <para>
	      The <application>gpg</application> command above might fail, the first time you
	      execute it. This happens, if <application>gpg</application> has never been
	      executed before for the root user. In this case, simply execute it a second time.
	    </para>
	  </note>
	  <para>
	    Then if you have <literal>Ubuntu/Xenial</literal> execute:
	    <screen>
<prompt>0 root@workstation ~ #</prompt>
<command>add-apt-repository 'deb http://repo.qlustar.com/repo/qluman 10.0-xenial main non-free'</command>
<prompt>0 root@workstation ~ #</prompt>
<command>add-apt-repository 'deb http://repo.qlustar.com/repo/qluman 10.0-xenial-proposed-updates main non-free'</command>
	    </screen>
	    After this you can install qluman-qt the usual way:
	    <screen>
<prompt>0 root@workstation ~ #</prompt>
<command>apt update</command>
<prompt>0 root@workstation ~ #</prompt>
<command>apt install qluman-qt</command>
	    </screen>
	  </para>
	  <note>
	    <para>
	      On Ubuntu you need to have the <literal>universe repository</literal> enabled in
	      your <literal>apt sources list</literal> for the above command to succeed.
	    </para>
	  </note>
	  <para>
	    Finally, the QluMan GUI can be launched as an ordinary user in a shell on the
	    workstation:
	    <screen>
<prompt>0 user@workstation ~ $</prompt>
<command>qluman-qt &amp;</command>
	    </screen>
	  </para>
	</section>
	<section xml:id="sec-QluMan-Workstation-container">
	  <title>Installing/running from a container</title>
	  <para>
	    <variablelist>
	      <varlistentry>
		<term>Singularity container</term>
		<listitem>
		  <para>
		    <link
		    xlink:href="http://singularity.lbl.gov/___blank___">Singularity</link>
		    container images with the QluMan GUI are available on our <link
		    xlink:href="https://qlustar.com/download___blank___">Download
		    page</link>. This makes it really easy to start the GUI as a non-root user
		    on any Linux machine with Singularity installed. Download the desired
		    version of the image and execute:
	    <screen>
<prompt>0 user@workstation ~ $</prompt>
<command>singularity exec <replaceable>&lt;path to singularity qluman image&gt;</replaceable> qluman-qt</command>
	    </screen>
		  </para>
		  <note>
		    <para>
		      Q-Leap hosts a <link
		      xlink:href="https://qlustar.com/docs/community-package-repo___blank___">community
		      package repository</link> with Singularity packages for various Debian
		      and Ubuntu versions. Given that you run one of the supported versions,
		      you can easily use this to install Singularity on your workstation.
		    </para>
		  </note>
		</listitem>
	      </varlistentry>
	      <varlistentry>
		<term>Docker container</term>
		<listitem>
		  <para>
		    A <link
		    xlink:href="https://hub.docker.com/r/qlustar/qluman-gui/___blank___">docker
		    container with the QluMan GUI</link> is available on the Docker Hub. You
		    will need the docker engine installed on your workstation to run it. To
		    install the docker engine for your platform, check the instructions at
		    <link
		    xlink:href="https://docs.docker.com/engine/installation/___blank___">the
		    official Docker site</link>. Once you have it installed, you can <link
		    xlink:href="https://qlustar.com/sites/default/files/downloads/run-qluman-docker___blank___">
		    download our run script</link> and execute it like (from the directory to
		    where you saved it)
		    <screen>
<prompt>0 user@workstation ~ $</prompt>
<command>chmod 755 run-qluman-docker</command>
<prompt>0 user@workstation ~ $</prompt>
<command>./run-qluman-docker</command>
		    </screen>
		    <note>
		      <para>
			The Docker container will most likely not be able to resolve the
			hostname of your cluster head-node. Hence, you will have to use its
			IP address in the <emphasis>Local</emphasis> field of <link
			xlink:href="../QluMan_Guide/index.html#sec-add-cluster___blank___">the
			connection info</link> dialog for it.
		      </para>
		    </note>
		    <note>
		      <para>
			If you always want to run the latest version of the QluMan GUI, start
			the script like <screen><command>./run-qluman-docker
			latest</command></screen>
		      </para>
		    </note>
		    <note>
		      <para>
			In case you are running the docker container as a non-root user and if
			that user has its home directory on an NFS mount point, carefully read
			and follow the instructions/output of the script.
		      </para>
		    </note>
		    <important>
		      <para>
			Be sure to verify the sha256 checksum of the
			<application>run-qluman-docker</application> script after the
			download. It is:
			1ee39af0953af315ba05494f0554efe905d7bb4c337573606b620edb4868de19
		      </para>
		    </important>
		  </para>
		</listitem>
	      </varlistentry>
	    </variablelist>
	    <important>
	      <para>
		The base version (e.g. 3.0.0) of the QluMan packages/containers on the
		workstation should be the same as on the head-node(s) to ensure correct
		operation. Some close unequal version combinations (e.g. 3.0.1.7 and 3.0.0.12)
		usually work too, but are not well tested.
	      </para>
	    </important>
	  </para>
	</section>
      </section>
    </section>
    <section xml:id="sec-Creating-Users">
      <title>Creating Users</title>
      <para>
	Authenticating users in the cluster can be done in many ways, hence the creation of
	users depends on what method is used. The most basic method is to use NIS. If there is
	no requirement of keeping user authentification in sync with some external service like
	e.g. <firstterm>LDAP</firstterm> this is sufficient. A NIS database is setup during the
	initial installation process and cluster users can be authenticated against
	it. Creating accounts that should use other authentification mechanisms is more complex
	and beyond the scope of this guide. Some options are explained in the admin manual. Add
	a test user by executing a command like this:
	<screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>adduser.sh -u test -n "Test User"</command>
	</screen>
	The behavior of the <application>adduser.sh</application> script can be customized in
	its configuration file <filename>/etc/qlustar/common/adduser.cf</filename>. It also
	contains the definition of the initial default user password.
      </para>
    </section>
    <section xml:id="sec-Compiling-MPI">
      <title>Compiling an MPI program</title>
      <para>
	<link xlink:href="http://www.mcs.anl.gov/research/projects/mpi/___blank___">MPI
	(Message Passing Interface)</link> is the de facto standard for distributed parallel
	programming on Linux clusters. The default <firstterm>MPI</firstterm> variant in
	Qlustar is <firstterm>OpenMPI</firstterm> and is automatically installed in the
	standard chroot during installation. You can test the correct installation of MPI with
	two small <emphasis>hello world</emphasis> test programs (one in C ,the other one in
	FORTRAN90) as the test user you created earlier: Login on the front-end node as this
	user and execute
	<screen>
<prompt>0 testuser@cl-front ~ $</prompt>
<command>mpicc.openmpi-gcc -o hello-world-c hello-world.c</command>
<prompt>0 testuser@cl-front ~ $</prompt>
<command>mpif90.openmpi-gcc -o hello-world-f hello-world.f90</command>
	</screen>
	After this you should have created two executables. Check it with
	<screen>
<prompt>0 testuser@cl-front ~ $</prompt>
<command>ls -l hello-world-?</command>
	</screen>
	Now we're prepared to test the queuing system with the two programs.
      </para>
    </section>
    <section xml:id="sec-Running-MPI">
      <title>Running an MPI Job</title>
      <para>
	Still logged in as the test user and assuming at least two demo nodes are started, we
	can submit the two "hello world" programs created previously as follows (commands are
	given for slurm):
	<screen>
<prompt>0 testuser@cl-front ~ $</prompt>
<command>salloc -N 2 --ntasks-per-node=2 --mem=20 -p demo srun hello-world-c</command>
	</screen>
	This will run the job interactively on 2 nodes with 2 processes each (total of 4
	processes). You should obtain an output like this:
	<screen>
salloc: Granted job allocation 3
Hello world from process 1 of 4
Hello world from process 2 of 4
Hello world from process 3 of 4
Hello world from process 0 of 4
salloc: Relinquishing job allocation 3
salloc: Job allocation 3 has been revoked.
	</screen>
      </para>
      <para>
	Similarly the F90 version can be submitted as a batch job using the script
	<filename>hello-world-f90-slurm.sh</filename> (to see the output, execute <command>cat
	slurm-<replaceable>&lt;job#&gt;</replaceable>.out</command> after the job has
	finished):
	<screen>
<prompt>0 testuser@cl-front ~ $</prompt>
<command>sbatch -N 2 --ntasks-per-node=2 --mem=20 -p demo hello-world-f90-slurm.sh</command>
	</screen>
      </para>
    </section>
    <section xml:id="sec-Running-Linpack">
      <title>Running the Linpack benchmark</title>
      <para>
	The <link xlink:href="http://www.top500.org/project/linpack/___blank___">Linpack
	benchmark</link> s used to classify supercomputers in the The <link
	xlink:href="http://www.top500.org/___blank___">Top 500</link> list. That's why on most
	clusters, it's probably run as one of the first parallel programs to check
	functionality, stability and performance. Qlustar comes with an optimized pre-compiled
	version of <firstterm>Linpack</firstterm> (using a current version of the <link
	xlink:href="http://www.openblas.net/___blank___">OpenBlas library</link>) , and a
	script to auto-generate the necessary input file given the number of nodes, processes
	per node and total amount of RAM for the run.
      </para>
      <para>
	The test user has some pre-defined shell aliases to simplify the submission of Linpack
	jobs. Type alias to see what's available. They are defined in
	<filename>$HOME/.bash/alias</filename>. Example submission (assuming you have 4 running
	demo nodes):
	<screen>
<prompt>0 testuser@cl-front ~ $</prompt>
<command>linp-4-demo-nodes</command>
	</screen>
	Check that the job is started (output should be similar):
	<screen>
<prompt>0 testuser@cl-front ~ $</prompt>
<command>squeue</command>
JOBID PARTITION     NAME     USER  ST  TIME  NODES  NODELIST(REASON)
   27      demo linstres     test   R  2:46      4     beo-[201-204]
	</screen>
	Now ssh to one of the nodes in the <literal>NODELIST</literal> and check with
	<application>top</application> that Linpack is running at full steam, like:
	<screen>
<prompt>0 testuser@cl-front ~ $</prompt>
<command>top</command>
  PID USER  PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                 
18307 test  20   0  354m 280m 2764 R  100 28.0   6:42.92 xhpl-openblas           
18306 test  20   0  354m 294m 2764 R   99 29.3   6:45.09 xhpl-openblas
	</screen>
	You can check the output of each Linpack run in the files:
	<filename>$HOME/bench/hpl/run/job-<replaceable>&lt;jobid&gt;</replaceable>-*/openblas/job-<replaceable>&lt;jobid&gt;</replaceable>-*-<replaceable>&lt;run#&gt;</replaceable>.out</filename>
	where <replaceable>&lt;jobid&gt;</replaceable> is the slurm <literal>JOBID</literal>
	(see the squeue command above) and <replaceable>&lt;run#&gt;</replaceable> is an
	integer starting from 1. The way the script is designed, it will run indefinitely,
	restarting Linpack in an infinite loop. So to stop it, you need to cancel the job like
	<screen>
<prompt>0 testuser@cl-front ~ $</prompt>
<command>scancel <replaceable>&lt;jobid&gt;</replaceable></command>
	</screen>
      </para>
    </section>
  </chapter>
  <!-- End of real content -->
  <xi:include href="Revision_History.xml"/>
  <index />
</book>
