<?xml version='1.0' encoding='utf-8' ?>
<!DOCTYPE section [
<!ENTITY % BOOK_ENTITIES SYSTEM "administration-manual.ent">
%BOOK_ENTITIES;
<!ENTITY % sgml.features "IGNORE">
<!ENTITY % xml.features "INCLUDE">
<!ENTITY % DOCBOOK_ENTS PUBLIC "-//OASIS//ENTITIES DocBook Character Entities V4.5//EN" "/usr/share/xml/docbook/schema/dtd/4.5/dbcentx.mod">
%DOCBOOK_ENTS;
]>
<section xmlns="http://docbook.org/ns/docbook"
 xmlns:xlink="http://www.w3.org/1999/xlink"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xml:id="admin-man-sect-stor-man">
  <title>Storage Management</title>
  <indexterm><primary>Storage Management</primary></indexterm>
  <section xml:id="admin-man-sect-raid">
    <title>Raid</title>
    <indexterm><primary>Storage Management</primary><secondary>Raid</secondary></indexterm>
    <section xml:id="admin-man-sect-kernel-sftw-raid">
      <title>Kernel Software RAID</title>
      <indexterm><primary>Storage Management</primary><secondary>Raid</secondary>
      <tertiary>Kernel Software Raid</tertiary></indexterm>
      <para>
	Software RAID is part of the Linux kernel. RAID configuration is done with the 
	<command>mdadm</command> command. It is used to manage the RAID devices (see man page).
	Status information is obtained form <filename>/proc/mdstat</filename>.
      </para>
      <variablelist>
	<varlistentry>
	  <term>How to replace a failed Disk in a Software RAID Setup</term>
	  <listitem>
	    <para>
	      In case of a disk failure use <command>mdadm</command> to remove the failed disk
	      from the raid-array, and after replacing the disk, first partition it as the old
	      one, then again use mdadm to include the new disk into the raid-array. A failed
	      disk is marked with a <literal>(F)</literal> in
	      <filename>/proc/mdstat</filename>.
	    </para>
	  </listitem>
	</varlistentry>
      </variablelist>
      <para>
	Example:
	<screen>
<prompt>0 root@cl-head ~ #</prompt><command>
cat /proc/mdstat</command>

Personalities : [raid0] [raid1] [raid5] [multipath]
read_ahead 1024 sectors
md0 : active raid1 sdb1[1] sda1[0]
104320 blocks [2/2] [UU]
md1 : active raid1 sdb2[0](F) sda2[1]
17414336 blocks [2/1] [_U]
md2 : active raid1 sdb3[1] sda3[0]
18322048 blocks [2/2] [UU]
unused devices: &lt;none&gt;
	</screen>
	So disk <filename>/dev/sdb</filename> has failed. In the example, the disk error
	affected only <filename>/dev/md1</filename>, but partitions of the faulty disk are also
	part of <filename>/dev/md0</filename> and <filename>/dev/md2</filename>. So they need
	to be removed as well before the disk can be replaced. Hence, the following commands
	need to be executed:
      </para>
      <para>
	To remove the faulty partition:
	<screen>
<prompt>0 root@cl-head ~ #</prompt><command>
mdadm --manage /dev/md1 -r /dev/sdb2</command>
	</screen>
	To mark the other affected partitions on the disk as faulty and remove them:
	<screen>
<prompt>0 root@cl-head ~ #</prompt><command>
mdadm --manage /dev/md0 -f /dev/sdb1</command>
<prompt>0 root@cl-head ~ #</prompt><command>
mdadm --manage /dev/md0 -r /dev/sdb1</command>
<prompt>0 root@cl-head ~ #</prompt><command>
mdadm --manage /dev/md2 -f /dev/sdb3</command>
<prompt>0 root@cl-head ~ #</prompt><command>
mdadm --manage /dev/md2 -r /dev/sdb3</command>
	</screen>
	Now the disk is not accessed any more and can be removed. After the new disk has been
	inserted, partition it like the other disk(s) using <filename>parted</filename> and
	then start the resync:
	<screen>
<prompt>0 root@cl-head ~ #</prompt><command>
mdadm --manage /dev/md0 -a /dev/sdb1</command>
<prompt>0 root@cl-head ~ #</prompt><command>
mdadm --manage /dev/md1 -a /dev/sdb2</command>
<prompt>0 root@cl-head ~ #</prompt><command>
mdadm --manage /dev/md2 -a /dev/sdb3</command>
	</screen>
	To watch the resync process, you can enter:
	<screen>
<prompt>0 root@cl-head ~ #</prompt><command>
watch --differences=cumulative cat /proc/mdstat</command> 
	</screen>
	Press <keycombo><keycap>Ctrl</keycap><keycap>C</keycap></keycombo> to exit the display.
      </para>
      <note>
	<para>
	  After an OS RAID boot disk has been replaced, one also has to make sure that the BIOS
	  will be able to boot from the new disk. For systems installed/booting in (U)EFI mode,
	  you have to execute the shell function <command>grub-install-uefi-raid</command> for
	  that. On systems running under legacy BIOS mode, you will have to run the command
	  <command>dpkg-reconfigure grub-pc</command> and make sure the new disk is selected
	  when prompted for the GRUB install locations (leave all other options as is).
	</para>
      </note>
    </section>
  </section>
  <section xml:id="admin-man-sect-vol-man">
    <title>Logical Volume Management</title>
    <indexterm><primary>Storage Management</primary>
    <secondary>Logical Volume Management</secondary></indexterm>
    <para>
      The Linux Logical Volume Manager (LVM) provides a convenient and flexible way of
      managing storage. Storage devices like hard disks or RAID sets are registered as
      <literal>physical volumes</literal>, and are then assigned to volume groups. 
      Volume groups contain one or more <literal>logical volume</literal>,
      which can be resized according to the storage space available in the
      volume group. New physical volumes can be added to or removed from a volume group
      at any time, thereby transparently enlarging or reducing the storage space available in
      a volume group. Filesystems are created on top of logical volumes.
    </para>
    <para>
      Examples:
      <screen>
<prompt>0 root@cl-head ~ #</prompt><command>
pvcreate /dev/sdb1</command>
<prompt>0 root@cl-head ~ #</prompt><command>
vgcreate vg0 /dev/sdb1</command>
<prompt>0 root@cl-head ~ #</prompt><command>
lvcreate -n scratch -L 1GB vg0</command>
      </screen>
      These commands declare <filename>/dev/sdb1</filename> as a physical volume, create the
      volume group vg0 with the physical volume <filename>/dev/sdb1</filename>, and create a
      logical volume <filename>/dev/vg0/scratch</filename> of size 1GB. You can now create a
      filesystem on this logical volume and mount it:
      <screen>
<prompt>0 root@cl-head ~ #</prompt><command>
mkfs.ext4 /dev/vg0/scratch</command>
<prompt>0 root@cl-head ~ #</prompt><command>
mount /dev/vg0/scratch /scratch</command>
      </screen>
      To increase the size of the filesystem, you do not have to unmount it, but you have to
      increase the logical volume before resizing the filesystem:
      <screen>
<prompt>0 root@cl-head ~ #</prompt><command>
lvextend -L +1G /dev/vg0/scratch</command>
<prompt>0 root@cl-head ~ #</prompt><command>
resize2fs /dev/vg0/scratch</command>
      </screen>
      This increased the filesystem by 1 GB. If you want to decrease the size of the
      filesystem, you first need to unmount it. After that, decrease the filesystem and finally
      reduce the logical volume:
      <screen>
<prompt>0 root@cl-head ~ #</prompt><command>
unmount /scratch</command>
<prompt>0 root@cl-head ~ #</prompt><command>
e2fsck -f /dev/vg0/scratch</command>
<prompt>0 root@cl-head ~ #</prompt><command>
resize2fs /dev/vg0/scratch 500M</command>
<prompt>0 root@cl-head ~ #</prompt><command>
lvreduce -L 500M /dev/vg0/scratch</command>
<prompt>0 root@cl-head ~ #</prompt><command>
mount /dev/vg0/scratch /scratch</command>
      </screen>
      This decreased the filesystem to 500Mb. To check how much space is left in a volume
      group, use the command <command>vgdisplay</command>. Look for a line showing
      <literal>Free Size</literal>.
    </para>
    <para>
      Frequent commands:
      <itemizedlist>
	<listitem>
	  <para>
	    Physical volumes: <command>pvcreate</command>
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Volume groups: <command>vgscan</command>, <command>vgchange</command>,
	    <command>vgdisplay</command>, <command>vgcreate</command>,
	    <command>vgremove</command>
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Logical volumes: <command>lvdisplay</command>,<command>lvcreate</command>,
	    <command>lvextend</command>, <command>lvreduce</command>,
	    <command>lvremove</command>
	  </para>
	</listitem>
      </itemizedlist>
    </para>
  </section>
  <xi:include href="zfs.xml" xmlns:xi="http://www.w3.org/2001/XInclude" />
</section>

<!--  LocalWords:  sdb
-->
