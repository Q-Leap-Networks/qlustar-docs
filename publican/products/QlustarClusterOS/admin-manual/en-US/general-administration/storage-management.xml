<?xml version='1.0'?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<section id="admin-man-sect-stor-man">
  <title>Storage Management</title>
  <indexterm><primary>Storage Management</primary></indexterm>
  <section id="admin-man-sect-raid">
    <title>Raid</title>
    <indexterm><primary>Storage Management</primary><secondary>Raid</secondary></indexterm>
    <section id="admin-man-sect-kernel-sftw-raid">
      <title>Kernel Software RAID</title>
      <indexterm><primary>Storage Management</primary><secondary>Raid</secondary>
      <tertiary>Kernel Software Raid</tertiary></indexterm>
      <para>
	Software RAID is part of the Linux kernel. RAID configuration is done with the 
	<command>mdadm</command> command. It is used to manage the RAID devices (see man page).
      Status information is obtained form <filename>/proc/mdstat.</filename>
      </para>
      <variablelist>
	<varlistentry>
	  <term>How to replace a failed Disk in a Software RAID Setup</term>
	  <listitem>
	    <para>
	      In case of a disk failure use <command>mdadm</command> to remove the failed disk
	      from the raid-array, and after replacing the disk first partition it as the old
	      one and again use mdadm to include the new disk into the raid-array. A failed 
	      disk is marked with a (F) in <filename>/proc/mdstat.</filename>
	    </para>
	  </listitem>
	</varlistentry>
      </variablelist>
      <para>
	Example:
      </para>
      <screen>
<prompt>$ </prompt>cat /proc/mdstat
Personalities : [raid0] [raid1] [raid5] [multipath]
read_ahead 1024 sectors
md0 : active raid1 sdb1[1] sda1[0]
104320 blocks [2/2] [UU]
md1 : active raid1 sdb2[0](F) sda2[1]
17414336 blocks [2/1] [_U]
md2 : active raid1 sdb3[1] sda3[0]
18322048 blocks [2/2] [UU]
unused devices: &lt;none&gt;
      </screen>
      <para>
	So disk <filename>/dev/sdb</filename> has failed. In the example, the disk error
	affected only /dev/md1, but partitions of the faulty disk are also part of
	<filename>/dev/md0</filename> and <filename>/dev/md2</filename>. So they need
	to be removed as well before the disk can be replaced. Hence, the following commands
	need to be executed:
      </para>
      <para>
	To remove the faulty partition:
      </para>
      <screen>
<command>
<prompt>$ </prompt>mdadm --manage /dev/md1 -r /dev/sdb2
</command>
      </screen>
      <para>
	To mark the other affected partitions on the disk as faulty and remove them:
      </para>
      <screen>
<command>
<prompt>$ </prompt> 
mdadm --manage /dev/md0 -f /dev/sdb1
</command>
<command>
<prompt>$ </prompt>
 mdadm --manage /dev/md0 -r /dev/sdb1
</command>
<command>
<prompt>$ </prompt> 
mdadm --manage /dev/md2 -f /dev/sdb3
</command>
<command>
<prompt>$ </prompt> 
mdadm --manage /dev/md2 -r /dev/sdb3
</command>
      </screen>
      <para>
	Now the disk is not accessed any more and can be removed. After the new disk has
	been inserted, repartition it:
      </para>
      <screen>
<command>
<prompt>$ </prompt>
sfdisk -d /dev/sda | sfdisk /dev/sdb
</command>
      </screen>
      <para>
	Now start the resync
      </para>
      <screen>
<command>
<prompt>$ </prompt>
mdadm --manage /dev/md0 -a /dev/sdb1
</command>
<command>
<prompt>$ </prompt>
mdadm --manage /dev/md1 -a /dev/sdb2
</command>
<command>
<prompt>$ </prompt>
mdadm --manage /dev/md2 -a /dev/sdb3
</command>
      </screen>
      <para>
	To watch the resync process, you can enter:
      </para>
      <screen>
<command>
<prompt>$ </prompt>
watch --differences=cumulative cat /proc/mdstat
</command> 
      </screen>
      <para>
	Press <keycombo><keycap>Ctrl</keycap><keycap>C</keycap></keycombo> to exit the display
      </para>
    </section>
  </section>
  <section id="admin-man-sect-vol-man">
    <title>Logical Volume Management</title>
    <indexterm><primary>Storage Management</primary>
    <secondary>Logical Volume Management</secondary></indexterm>
    <para>
      The Linux Logical Volume Manager (LVM) provides a convenient and flexible way of
      managing storage. Storage devices like hard discs or RAID sets are registered as
      <replaceable>physical volumes</replaceable>, and are then assigned to volume groups. 
      Volume groups contain one or more <replaceable>logical volumes</replaceable>,
      which can be resized according to the storage space available in the
      volume group. New physical volumes can be added to or removed from a volume group
      at any time, thereby transparently enlarging or reducing the storage space available in
      a volume group. Filesystems are created on top of logical volumes.
    </para>
    <para>
      Examples:
    </para>
    <screen>
<command>
<prompt>$ </prompt>
pvcreate /dev/sdb1
</command>
<command>
<prompt>$ </prompt>
vgcreate vg0 /dev/sdb1
</command>
<command>
<prompt>$ </prompt>
lvcreate -n scratch -L 1GB vg0
</command>
    </screen>
    <para>
      These commands declare /dev/sdb1 as a physical volume, create the volume group
      vg0 with the physical volume /dev/sdb1, and create a logical volume
      /dev/vg0/scratch of size 1GB. You can now create a filesystem on this logical volume
      and mount it:
    </para>
    <screen>
<command>
<prompt>$ </prompt>
mkfs.ext4 /dev/vg0/scratch
</command>
<command>
<prompt>$ </prompt>
mount /dev/vg0/scratch /scratch
</command>
    </screen>
    <para>
      To increase the size of the filesystem you do not have to unmount it but you have to
      increase the logical volume before resizing the filesystem:
    </para>
    <screen>
<command>
<prompt>$ </prompt>
lvextend -L +1G /dev/vg0/scratch
</command>
<command>
<prompt>$ </prompt>
resize2fs /dev/vg0/scratch
</command>
    </screen>
    <para>
      This increased the filesystem by 1 Gb. If you want to decrease the size of the filesystem
      you first need to unmount it. After that decrease the filesystem and finally reduce the
      logical volume:
    </para>
    <screen>
<command>
<prompt>$ </prompt>
unmount /scratch
</command>
<command>
<prompt>$ </prompt>
e2fsck -f /dev/vg0/scratch
</command>
<command>
<prompt>$ </prompt>
resize2fs /dev/vg0/scratch 500M
</command>
<command>
<prompt>$ </prompt>
lvreduce -L 500M /dev/vg0/scratch
</command>
<command>
<prompt>$ </prompt>
mount /dev/vg0/scratch /scratch
</command>
    </screen>
    <para>
      This decreased the filesystem to 500Mb. To check how much space is left in a volume
      group use the command <command>vgdisplay</command>. Look for a line showing Free Size.
    </para>
    <para>
      Frequent commands:
    </para>
    <simplelist>
      <member>Physical volumes: <command>pvcreate</command></member>
      <member>Volume groups: <command>vgscan</command>, <command>vgchange</command>,
      <command>vgdisplay</command>, <command>vgcreate</command>, 
      <command>vgremove</command></member>
      <member>Logical volumes: <command>lvdisplay</command>,<command>lvcreate</command>,
      <command>lvextend</command>, <command>lvreduce</command>,
      <command>lvremove</command></member>
    </simplelist>
  </section>
  <xi:include href="zfs.xml" xmlns:xi="http://www.w3.org/2001/XInclude" />
</section>
