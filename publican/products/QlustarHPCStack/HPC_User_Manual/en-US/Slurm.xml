<?xml version='1.0' encoding='utf-8' ?>
<!DOCTYPE chapter [
<!ENTITY % BOOK_ENTITIES SYSTEM "HPC_User_Manual.ent">
%BOOK_ENTITIES;
<!ENTITY % sgml.features "IGNORE">
<!ENTITY % xml.features "INCLUDE">
<!ENTITY % DOCBOOK_ENTS PUBLIC "-//OASIS//ENTITIES DocBook Character Entities V4.5//EN" "/usr/share/xml/docbook/schema/dtd/4.5/dbcentx.mod">
%DOCBOOK_ENTS;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
      xmlns:xlink="http://www.w3.org/1999/xlink"
      xmlns:xi="http://www.w3.org/2001/XInclude"
      xml:id="chap-hpc-user-man-slurm">
  <title>Using the Slurm workload manager</title>
  <section xml:id="sec-Introduction">
    <title>Job Submission</title>
    <para>
      Use the <command>sbatch</command> command to submit a batch
      script.
    </para>
    <para>
      Important sbatch flags:
      <variablelist>
	<varlistentry>
	  <term>--partition=partname</term>
	  <listitem>
	    <para>
	      Job to run on partition 'partname'. (default: 'norm')
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>--ntasks=#</term>
	  <listitem>
	    <para>
	      Number of tasks (processes) to be run
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>--cpus-per-task=#</term>
	  <listitem>
	    <para>
	      Number of CPUs required for each task (e.g. '8' for an
	      8-way multithreaded job)
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>--ntasks-per-core=1</term>
	  <listitem>
	    <para>
	      Do not use hyperthreading (this flag typically used for
	      parallel jobs)
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>--mem=#g</term>
	  <listitem>
	    <para>
	      Memory required for the job
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>--exclusive</term>
	  <listitem>
	    <para>
	      Allocate the node exclusively
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>--no-requeue | --requeue</term>
	  <listitem>
	    <para>
	      If an allocated node hangs, whether the job should be
	      requeued or not.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>--error=/path/to/dir/filename</term>
	  <listitem>
	    <para>
	      Location of stderr file (by default, slurm######.out in
	      the submitting directory)
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>--output=/path/to/dir/filename</term>
	  <listitem>
	    <para>
	      Location of stdout file (by default, slurm######.out in
	      the submitting directory)
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>--ignore-pbs</term>
	  <listitem>
	    <para>
	      Ignore PBS directives in batch scripts. (by default,
	      Slurm will try to utilize all PBS directives)
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term>--license=matlab</term>
	  <listitem>
	    <para>
	      Request a Matlab license
	    </para>
	  </listitem>
	</varlistentry>
      </variablelist>
      More useful flags and environment variables are detailed at the
      <link
	  xlink:href="https://hpc.nih.gov/docs/pbs2slurm.html">PBS
      to Slurm webpage.</link>
    </para>
    <para>
      <variablelist>
	<varlistentry>
	  <term>Single-threaded batch job</term>
	  <listitem>
	    <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>sbatch jobscript</command>
	    </screen>
	    <para>
	    This job will be allocated 2 CPUs and 4 GB of memory.
	    </para>
	  </listitem>
	</varlistentry>
      <varlistentry>
	<term>Multi-threaded batch job</term>
	<listitem>
	  <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>sbatch --cpus-per-task=# jobscript</command>
	  </screen>
	  <para>
	    The above job will be allocated '#' CPUs, and (# * 2) GB of
	    memory. e.g. with --cpus-per-task=4, the default memory
	    allocation is 8 GB of memory.
	  </para>
	  <para>
	    You should use the Slurm environment variable
	    <envar>$SLURM_CPUS_PER_TASK</envar> within your script to specify the number
	    of threads to the program. For example, to run a Novoalign job
	    with 8 threads, set up a batch script like this:
	    <screen>
#!/bin/bash

module load novocraft
novoalign -c $SLURM_CPUS_PER_TASK  -f s_1_sequence.txt -d celegans -o SAM > out.sam
	    </screen>
	    and submit with:
	    <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>sbatch --cpus-per-task=8  jobscript</command>
	    </screen>
	    <note>
	      <para>
		When jobs are submitted without specifying the number
		of CPUs per task explicitly the <envar>$SLURM_CPUS_PER_TASK</envar>
		environment variable is not set.
	      </para>
	    </note>
	  </para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>Allocating more memory:</term>
	<listitem>
	  <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>sbatch --mem=#g jobscript</command>
	  </screen>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>Exclusively allocating nodes</term>
	<listitem>
	  <para>
	    Add <parameter>--exclusive</parameter> to your sbatch command
	    line to exclusively allocate a node. Note that the batch system
	    will still limit you to the requested CPUs and memory, or to the
	    default 2 CPUs and 4 GB if you do not specifically request CPUs and memory.
	  </para>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>Auto-threading apps</term>
	<listitem>
	  <para>
	    Programs that 'auto-thread' (i.e. attempt to use all available
	    CPUs on a node) should be run with the
	    <parameter>--exclusive</parameter> flag. This will give you an
	    exclusively allocated node with at least 16 CPUs.
	  </para>
	</listitem>
      </varlistentry>
      </variablelist>
      A major change in the new cluster is that the batch system will
      <emphasis role="bold">allocate by core</emphasis>, rather than
      by node. Thus, your job may be
      allocated 4 cores and 8 GB of memory on a node which has 16
      cores and 32 GB of memory. O ther jobs may be utilizing the
      remaining 12 cores and 24 GB of memory, so that your jobs may
      not have exclusive use of the
      node. <application>Slurm</application> will not allow any job
      to utilize more memory or cores than were allocated.
    </para>
    <para>
      The default <application>Slurm</application> allocation is 1
      physical core (2 CPUs) and 4 GB of memory. For any jobs that
      require more memory or CPU, you need to specify these
      requirements when submitting the job.
    </para>
  </section>
  <section xml:id="sec-parallel-jobs">
    <title>Parallel Jobs</title>
    <para>
      For submitting parallel jobs, a few rules have to be understood
      and followed. In general they depend on the type of
      parallelization and the architecture.
    </para>
    <variablelist>
      <varlistentry>
	<term>OpenMP Jobs</term>
	<listitem>
	  <para>
	    An SMP-parallel job can only run within a node, so it is
	    necessary to include the options <parameter>-N
	    1</parameter> and <parameter>-n 1</parameter>. The maximum
	    number of processors for an SMP-parallel program is 488 on
	    Venus and 56 on taurus (smp island). Using
	    <parameter>--cpus-per-task N SLURM</parameter> will start
	    one task and you will have N CPUs available for your
	    job. An example job file would look like:
	  </para>
	  <screen>
#!/bin/bash
#SBATCH -J Science1
#SBATCH --nodes=1
#SBATCH --tasks-per-node=1
#SBATCH --cpus-per-task=8
#SBATCH --mail-type=end
#SBATCH --mail-user=your.name@tu-dresden.de
#SBATCH --time=08:00:00

export OMP_NUM_THREADS=8
./path/to/binary
	  </screen>
	</listitem>
      </varlistentry>
      <varlistentry>
	<term>MPI Jobs</term>
	<listitem>
	  <para>
	    For MPI jobs one typically allocates one core per task
	    that has to be started.
	    <note>
	      <para>
		There are different MPI libraries on Taurus and Venus,
		so you have to compile the binaries specifically for
		their target.
	      </para>
	    </note>
	  </para>
	  <screen>
#!/bin/bash
#SBATCH -J Science1
#SBATCH --ntasks=864
#SBATCH --mail-type=end
#SBATCH --mail-user=your.name@tu-dresden.de
#SBATCH --time=08:00:00

srun ./path/to/binary
	  </screen>
	</listitem>
      </varlistentry>
    </variablelist>
  </section>
  <section xml:id="sec-allocating-GPUs">
    <title>Allocating GPUs</title>
    <para>
      To make use of GPUs, jobs have to be submitted to the gpu
      partition and specifically request the type and number of
      GPUs. For example:
    </para>
    <screen>
# request one GPU
<prompt>0 root@cl-head ~ #</prompt>
<command>sbatch --partition=gpu --gres=gpu:k20x:1 script.sh</command>
# request two GPUs
<prompt>0 root@cl-head ~ #</prompt>
<command>sbatch --partition=gpu --gres=gpu:k20x:2 script.sh</command>
    </screen>
    <para>
      The request for the GPU resource is in the form
      <emphasis role="bold">resourceName:resourceType:number</emphasis>.
    </para>
    <para>
      The maximal number of GPUs available per node is currently 2.
    </para>
    <para>
      To allocate a GPU for an interactive session, e.g. to compile a
      program, use:
    </para>
    <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>sinteractive --constraint=gpuk20x --gres=gpu:k20x:1</command>
    </screen>
    <para>
      or use:
    </para>
    <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>sinteractive --constraint=gpum2050 --gres=gpu:m2050:1 </command>
    </screen>
  </section>
  <section xml:id="sec-interactive-jobs">
    <title>INTERACTIVE JOBS</title>
    <para>
      Interactive activities like editing, compiling etc. are normally
      limited to the login nodes. For longer interactive sessions you
      can allocate cores on the compute node with the command
      <command>salloc</command>. It takes the same options like
      <command>sbatch</command> to specify the required resources.
    </para>
    <para>
      The difference to <command>LSF</command> is, that
      <command>salloc</command> returns a new shell on the node, where
      you submitted the job. You need to use the command
      <command>srun</command> in front of the following commands to
      have these commands executed on the allocated resources. If you
      allocate more than one task, please be aware that
      <command>srun</command> will run the command on each allocated task!
    </para>
    <para>
      An example of an interactive session looks like:
    </para>
    <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>srun --pty -n 1 -c 4 --time=1:00:00 --mem-per-cpu=1700 bash</command>
srun: job 13598400 queued and waiting for resources
srun: job 13598400 has been allocated resources
<prompt>0 root@cl-head ~ #</prompt>
<command>start interactive work with e.g. 4 cores.</command>
    </screen>
  </section>
  <section xml:id="sec-walltime-limits">
    <title>Walltime Limits</title>
    <para>
      Most partitions have walltime limits. Use
      <command>batchlim</command> to see the default and max walltime
      limits for each partition.
    </para>
    <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>batchlim</command>
Partition        MaxCPUsPerUser     DefWalltime     MaxWalltime
---------------------------------------------------------------
norm                  1024           04:00:00       10-00:00:00
interactive             64           08:00:00        1-12:00:00
largemem               128           04:00:00       10-00:00:00
ibfdr                  512        10-00:00:00       10-00:00:00
gpu                    128        10-00:00:00       10-00:00:00
unlimited              128          UNLIMITED         UNLIMITED
niddk                  512           04:00:00       10-00:00:00
    </screen>
    <para>
      If no walltime is requested on the command line, the walltime
      set for the job will be the default walltime in the table above.
      To request a specific walltime, use the
      <parameter>--time</parameter> option to
      <command>sbatch</command>. For example:
    </para>
    <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>sbatch  --time=24:00:00  jobscript</command>
    </screen>
    <para>
      will submit a job to the norm partition, and request a walltime
      of 24 hrs. If the job runs over 24 hrs it will be killed by the
      batch system.
    </para>
    <para>
      To see the walltime limits and current runtimes for jobs, you
      can use the <command>squeue</command> command.
    </para>
    <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>squeue -O jobid,timelimit,timeused -u  username</command>
JOBID               TIME_LIMIT          TIME
1418444             10-00:00:00         5-05:44:09
1563535             5-00:00:00          1:35:12
1493019             3-00:00:00          2-17:03:27
1501256             5-00:00:00          2-03:08:42
1501257             5-00:00:00          2-03:08:42
1501258             5-00:00:00          2-03:08:42
1501259             5-00:00:00          2-03:08:42
1501260             5-00:00:00          2-03:08:42
1501261             5-00:00:00          2-03:08:42
    </screen>
    <para>
      For many more <command>squeue</command> options, see the squeue man page.
    </para>
  </section>
  <section xml:id="sec-licenses">
    <title>Licenses</title>
    <para>
      Several licensed software products are available on the cluster,
      including <application>Matlab</application> and
      <application>Mathematica</application>. To use licensed software
      in your batch job, you must specify the
      <parameter>--license</parameter> flag when submitting your
      job. This flag ensures that the batch system will wait until a
      license is available before starting the job. If you do not
      specify this flag, there is a risk that the batch system will
      start your job, the job will then be unable to get a license,
      and will exit immediately.
    </para>
    <para>
      Example:
    </para>
    <screen>
sbatch --license=matlab  jobscript               (request a single Matlab license)
sbatch --license=matlab,matlab-stat  jobscript   (request a Matlab + Matlab Stats Toolbox license)
sinteractive --license=matlab-pde                (interactive job that needs a Matlab license)
    </screen>
    <para>
      The current availability of licenses can be seen on the 
      <link
	  xlink:href="https://hpc.nih.gov/systems/status/index.html#licenses">Systems
      Status</link>  page, or by typing <command>licenses</command> on the command line.
    </para>
  </section>
  <section xml:id="sec-deleting-jobs">
    <title>Deleting Jobs</title>
    <para>
      The <command>scancel</command> command is used to delete
      jobs. Examples:
    </para>
    <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>scancel 232323</command>
(delete job 232323)

<prompt>0 root@cl-head ~ #</prompt>
<command>scancel -u username</command>
(delete all jobs belonging to user)

<prompt>0 root@cl-head ~ #</prompt>
<command>scancel --name=JobName</command>
(delete job with the name JobName)

<prompt>0 root@cl-head ~ #</prompt>
<command>scancel --state=PENDING</command>
(delete all PENDING jobs)

<prompt>0 root@cl-head ~ #</prompt>
<command>scancel --state=RUNNING</command>
(delete all RUNNING jobs)

<prompt>0 root@cl-head ~ #</prompt>
<command>scancel --nodelist=cn0005</command>
(delete any jobs running on node cn0005)
    </screen>
  </section>
  <section xml:id="sec-job-states">
    <title>Job States</title>
    <para>
      Common job states:
    </para>
    <informaltable frame="all">
      <tgroup cols="2">
	<thead>
	  <row>
	    <entry>Job State Code</entry>
	    <entry>Means</entry>
	  </row>
	</thead>
	<tbody>
	  <row>
	    <entry>R</entry>
	    <entry>Running</entry>
	  </row>
	  <row>
	    <entry>PD</entry>
	    <entry>Pending (Queued). Some possible reasons:
	    QOSMaxCpusPerUserLimit (User has reached the maximum
	    allocation) Dependency (Job is dependent on another job
	    which has not completed) Resources (currently not enough
	    resources to run the job) Licenses (job is waiting for a
	    license, e.g. Matlab)</entry>
	  </row>
	  <row>
	    <entry>CG</entry>
	    <entry>Completing</entry>
	  </row>
	  <row>
	    <entry>CA</entry>
	    <entry>Cancelled</entry>
	  </row>
	  <row>
	    <entry>F</entry>
	    <entry>Failed</entry>
	  </row>
	  <row>
	    <entry>TO</entry>
	    <entry>Timeout</entry>
	  </row>
	  <row>
	    <entry>NF</entry>
	    <entry>Node failure</entry>
	  </row>
	</tbody>
      </tgroup>
    </informaltable>
    <para>
      Use the <command>sacct</command> command to check on the states
      of completed jobs.
    </para>
    <para>
      Show all your jobs in any state since midnight:
    </para>
    <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>sacct</command>
    </screen>
    <para>
      Show all jobs that failed since midnight
    </para>
    <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>sacct --state f</command>
    </screen>
    <para>
      Show all jobs that failed this month
    </para>
    <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>sacct --state f --starttime 2016-03-01 </command>
    </screen>
  </section>
  <section xml:id="sec-exit-codes">
    <title>Exit codes</title>
    <para>
      The completion status of a job is essentially the exit status of
      the job script with all the complications that entails. For
      example take the following job script:
    </para>
    <screen>
#! /bin/bash

module load GATK/2.3.4
GATK -m 5g -T RealignerTargetCreator ...
echo "DONE"
    </screen>
    <para>
      This script tries to load a non-existent
      <application>GATK</application> version and then calls
      <application>GATK</application>. This will fail. However, bash
      by default keeps executing even if commands fail, so the script
      will eventually print 'DONE'. Since the exit status of a bash
      script is the exit status of the last command and echo returns 0
      (SUCCESS), the script as a whole will exit with an exit code of
      0, signalling sucess and the job state will show COMPLETED since
      SLURM uses the exit code to judge if a job completed
      sucessfully.
    </para>
    <para>
      Similarly, if a command in the middle of the job script were
      killed for exceeding memory, the rest of the job script would
      still be executed and could potentially return an exit code of 0
      (SUCCESS), resulting again in a state of COMPLETED.
    </para>
    <para>
      Conversely, in the following example a sucessful analysis is
      followed by a command that fails
    </para>
    <screen>
#! /bin/bash
module load GATK/3.4.0
GATK -m 5g -T RealignerTargetCreator ...
touch /file/in/non/existing/directory/DONE
    </screen>
    <para>
      Even though the actual analysis (here the
      <application>GATK</application> call) finished sucessfully, the
      last command will fail, resulting in a final state of FAILED for
      the batch job.
    </para>
    <para>
      Some defensive bash programming techniques can help ensure that
      a job script will show a final state of FAILED if anything goes
      wrong.
    </para>
    <bridgehead>Use set -e</bridgehead>
    <para>
      Starting a bash script with <parameter>set -e</parameter> will
      tell bash to stop executing a script if a command fails and
      signal failiure with a non-zero exit code which will be
      reflected as a FAILED state in <application>SLURM</application>.
    </para>
    <screen>
#! /bin/bash
set -e

module load GATK/3.4.0
GATK -m 5g -T RealignerTargetCreator ...
echo "DONE"
    </screen>
    <para>
      One complication with this approach is that some commands will
      return non-zero exit codes. For example grepping for a string
      that does not exist.
    </para>
    <bridgehead>Check errors for individual commands</bridgehead>
    <para>
     A more selective approach involves carefully checking the exit
     codes of the important parts of a job script. This can be done
     with conventional if/else statements or with conditional short
     circuit evaluation often seen in scripts. For example:
    </para>
    <screen>
#! /bin/bash

function fail {
    echo "FAIL: $@" >&amp;2
    exit 1  # signal failure
}

module load GATK/3.4.0 || fail "Could not load GATK module"
GATK -m 5g -T RealignerTargetCreator ... || fail "RealignerTargetCreator failed"
echo "DONE"
    </screen>
  </section>
  <section xml:id="sec-array-jobs">
    <title>Array Jobs</title>
    <para>
      Array jobs can be used to create a sequence of jobs that share
      the same executable and resource requirements, but have
      different input files, to be submitted, controlled, and
      monitored as a single unit. The arguments
      <parameter>-a</parameter> or <parameter>--array</parameter> take
      an additional parameter that specify the array indices. Within
      the job you can read the environment variables
      <envar>SLURM_ARRAY_JOB_ID</envar>, which will be set to the
      first job ID of the array, and
      <envar>SLURM_ARRAY_TASK_ID</envar>, which will be set
      individually for each step.
    </para>
    <para>
      Within an array job, you can use <varname>%a</varname> and
      <varname>%A</varname> in addition to <varname>%j</varname> and
      <varname>%N</varname> (described above) to make the output file
      name specific to the job. <varname>%A</varname> will be replaced
      by the value of <envar>SLURM_ARRAY_JOB_ID</envar> and
      <varname>%a</varname> will be replaced by the value of
      <envar>SLURM_ARRAY_TASK_ID</envar>.
    </para>
    <para>
      Here is an example how an array job can looks like:
    </para>
    <screen>
#!/bin/bash
#SBATCH -J Science1
#SBATCH --array 0-9
#SBATCH -o arraytest-%A_%a.out
#SBATCH -e arraytest-%A_%a.err
#SBATCH --ntasks=864
#SBATCH --mail-type=end
#SBATCH --mail-user=your.name@tu-dresden.de
#SBATCH --time=08:00:00

echo "Hi, I am step $SLURM_ARRAY_TASK_ID in this array job $SLURM_ARRAY_JOB_ID"
    </screen>
    <para>
      For further details please read the Slurm documentation.
    </para>
  </section>
  <section xml:id="sec-chain-jobs">
    <title>Chain Jobs</title>
    <para>
      You can use <firstterm>chain jobs</firstterm> to create
      dependencies between jobs. This is often the case if a job
      relies on the result of one or more preceding jobs. Chain jobs
      can also be used if the runtime limit of the batch queues is not
      sufficient for your job. <application>SLURM</application> has an
      option <parameter>-d</parameter> or
      <parameter>--dependency</parameter> that allows to specify that
      a job is only allowed to start if another job finished.
    </para>
    <para>
      Here is an example how a chain job can looks like, the example
      submits 4 jobs (described in a job file) that will be executed
      on after each other with different CPU numbers:
    </para>
    <screen>
#!/bin/bash
TASK_NUMBERS="1 2 4 8"
DEPENDENCY=""
JOB_FILE="myjob.slurm"

for TASKS in $TASK_NUMBERS ; do
    JOB_CMD="sbatch --ntasks=$TASKS"
    if [ -n "$DEPENDENCY" ] ; then
        JOB_CMD="$JOB_CMD --dependency afterany:$DEPENDENCY"
    fi
    JOB_CMD="$JOB_CMD $JOB_FILE"
    echo -n "Running command: $JOB_CMD  "
    OUT=`$JOB_CMD`
    echo "Result: $OUT"
    DEPENDENCY=`echo $OUT | awk '{print $4}'`
done
    </screen>
  </section>
  <section xml:id="sec-monitoring-jobs">
    <title>Monitoring Jobs</title>
    <para>
      <command>squeue</command> will report all jobs on the
      cluster. <command>squeue -u username</command> will report your
      running jobs. Slurm commands like squeue are very flexible, so
      that you can easily create your own aliases.
    </para>
    <para>
      Example of squeue:
    </para>
    <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>squeue</command>
JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
22392     ibfdr meme_sho   susanc PD       0:00      1 (Dependency)
22393     ibfdr meme_sho   susanc PD       0:00      1 (Dependency)
22404     ibfdr   stmv-1   susanc  R      10:04      1 cn0414
22391     ibfdr meme_sho   susanc  R      10:06      1 cn0413
    </screen>
  </section>
  <section xml:id="sec-email-notifications">
    <title>Email notifications</title>
    <para>
      Using the <parameter>--mail-type=&lt;type&gt;</parameter> option
      to <command>sbatch</command>, users can request email
      notifications from <application>SLURM</application> as certain
      events occur. Multiple event types can be specified as a comma
      separated list. For example:
    </para>
    <screen>
<prompt>0 root@cl-head ~ #</prompt>
<command>sbatch --mail-type=BEGIN,TIME_LIMIT_90,END batch_script.sh</command>
    </screen>
    <para>
      Available event types:
    </para>
    <informaltable frame="all">
      <tgroup cols="2">
	<thead>
	  <row>
	    <entry>Event type</entry>
	    <entry>Description</entry>
	  </row>
	</thead>
	<tbody>
	  <row>
	    <entry>BEGIN</entry>
	    <entry>Job started</entry>
	  </row>
	  <row>
	    <entry>END</entry>
	    <entry>Job finished</entry>
	  </row>
	  <row>
	    <entry>FAIL</entry>
	    <entry>Job failed</entry>
	  </row>
	  <row>
	    <entry>REQUEUE</entry>
	    <entry>Job was requeued</entry>
	  </row>
	  <row>
	    <entry>ALL</entry>
	    <entry>BEGIN,END,FAIL,REQUEUE</entry>
	  </row>
	  <row>
	    <entry>TIME_LIMIT_50</entry>
	    <entry>Job reached 50% of its time limit</entry>
	  </row>
	  <row>
	    <entry>TIME_LIMIT_80</entry>
	    <entry>Job reached 80% of its time limit</entry>
	  </row>
	  <row>
	    <entry>TIME_LIMIT_90</entry>
	    <entry>Job reached 90% of its time limit</entry>
	  </row>
	  <row>
	    <entry>TIME_LIMIT</entry>
	    <entry>Job reached its time limit</entry>
	  </row>
	</tbody>
      </tgroup>
    </informaltable>
  </section>
</chapter>

